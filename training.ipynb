{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77fd8c30-ad01-41ef-9e06-8fa3370dec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f18fe2-75cf-4a38-9b35-966143e3e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('WikiQASent.pos.ans.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da0e23-cec0-4c89-95d8-d0c1ded52bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477f4fcf-6712-4733-b00e-dbc6c0c97ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = df[['QuestionID', 'Question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd903cdd-9816-41e4-b1a3-53ac1dccf67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionID</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q0</td>\n",
       "      <td>HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>how are glacier caves formed?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q4</td>\n",
       "      <td>how a water pump works</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q11</td>\n",
       "      <td>how big is bmc software in houston, tx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q11</td>\n",
       "      <td>how big is bmc software in houston, tx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q16</td>\n",
       "      <td>how much is 1 tablespoon of water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q16</td>\n",
       "      <td>how much is 1 tablespoon of water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q16</td>\n",
       "      <td>how much is 1 tablespoon of water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q17</td>\n",
       "      <td>how much are the harry potter movies worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q18</td>\n",
       "      <td>how a rocket engine works</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Q20</td>\n",
       "      <td>how old was sue lyon when she made lolita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Q21</td>\n",
       "      <td>how are cholera and typhus transmitted and pre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QuestionID                                           Question\n",
       "0          Q0    HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US\n",
       "1          Q1                      how are glacier caves formed?\n",
       "2          Q4                             how a water pump works\n",
       "3         Q11             how big is bmc software in houston, tx\n",
       "4         Q11             how big is bmc software in houston, tx\n",
       "5         Q16                  how much is 1 tablespoon of water\n",
       "6         Q16                  how much is 1 tablespoon of water\n",
       "7         Q16                  how much is 1 tablespoon of water\n",
       "8         Q17         how much are the harry potter movies worth\n",
       "9         Q18                          how a rocket engine works\n",
       "10        Q20          how old was sue lyon when she made lolita\n",
       "11        Q21  how are cholera and typhus transmitted and pre..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade6fc4-2f79-4e07-b8a0-c41087c5842b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NLP Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84491f9f-2381-462f-95fd-4c947a04d0fa",
   "metadata": {},
   "source": [
    "1. Tokenization\n",
    "2. Lower + stem\n",
    "3. exclude punctuation characters\n",
    "4. Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a8cfa11-ff43-4cb0-9fed-8072b4d074ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#For first time\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ddef6b-80c1-4779-b026-6b86100d3adf",
   "metadata": {},
   "source": [
    "### Tokenization & remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "987c7aaa-6423-4484-8dd2-934b84069bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d7826-fdb2-4961-9342-d14027887c31",
   "metadata": {},
   "source": [
    "### Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe087fcf-e7c3-4fa8-8b50-dbc5eb64c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea94c3f-c206-41d6-987e-6ff9d610c29e",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f24c5d8-6bc5-4dc2-9a43-a53d1d518619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokenized_sentence, all_words):\n",
    "    tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
    "    bag = np.zeros(len(all_words), dtype=np.float32)\n",
    "    \n",
    "    for idex, w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idex] = 1.0\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf53ab-0b37-4042-8977-3355f9afc0c4",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb2f05-390b-4415-9bec-50e3fcdd9dcb",
   "metadata": {},
   "source": [
    "#### AllWords & QTag & InOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "299a3c7a-5345-4b86-8f48-b4a0fe3840f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "Qtags = []\n",
    "XY = []\n",
    "\n",
    "for question in data_set.values:\n",
    "    Qtags.append(question[0])\n",
    "    all_words.extend(tokenize(question[1]))\n",
    "    XY.append((tokenize(question[1]), question[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a843ad35-0042-4cf6-bca0-4967586c6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [stem(w) for w in all_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d17e40e8-c731-4493-bdb1-54e4057bc453",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915893c-5ba7-4c86-8671-ce49648fc85d",
   "metadata": {},
   "source": [
    "#### X-Train Y-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "40b09954-9640-4d4c-833b-fc55a9204deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6034ea9e-5482-4b9e-875d-37aeca13cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, tag in XY:\n",
    "    bag = bag_of_words(sentence, all_words)\n",
    "    x_train.append(bag)\n",
    "    \n",
    "    label = Qtags.index(tag)\n",
    "    y_train.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ae9b5bbd-a0f7-45ee-a43a-71ae08d35a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "195386f5-9530-455d-8ed2-1f22d483cbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train) == len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8672018-6f2e-4ba5-8792-292ea572a2b6",
   "metadata": {},
   "source": [
    "#### Chat-Data-Set Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f8d5d0f7-4dfd-458d-a79d-d05b38324122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(y_train)\n",
    "        self.x_data = x_train\n",
    "        self.y_data = y_train\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567cae98-caed-4d8b-941f-fa38272cc2f3",
   "metadata": {},
   "source": [
    "#### Nerual Netword Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f237ffd-cb83-4be7-9661-b9d11318526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerualNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NerualNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc53c07-5d6a-4d46-ab84-cfaba8b9809a",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efa23f-41ec-4e29-b9bd-f709b7de7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "hidden_size = 8\n",
    "output_size = len(Qtags)  # Number of different class\n",
    "input_size = len(x_train[0])  # Number of the bag of words len(all_words)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "dataset = ChatDataSet()\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = NerualNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch_ in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward\n",
    "        outputs = model(words)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch_ + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch_ + 1}/{num_epochs}, loss={loss.item():.4f}\")\n",
    "\n",
    "print(f\"Final loss, loss={loss.item():.4f}\")\n",
    "\n",
    "data_ = {\"model_state\": model.state_dict, \"input_size\": input_size, \"output_size\": output_size, \"hidden_size\": hidden_size, \"all_words\": all_words, \"tags\": Qtags}\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data_, FILE)\n",
    "\n",
    "\n",
    "print(f\"training complete. file saved to {FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0defa55d-5da4-4818-b1cd-d0bcd7a7c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[nltk_data] Downloading package punkt to\n",
    "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
    "[nltk_data]   Package punkt is already up-to-date!\n",
    "True\n",
    "Epoch 100/1000, loss=0.0240\n",
    "Epoch 200/1000, loss=0.0000\n",
    "Epoch 300/1000, loss=0.0002\n",
    "Epoch 400/1000, loss=0.0000\n",
    "Epoch 500/1000, loss=0.0000\n",
    "Epoch 600/1000, loss=0.0000\n",
    "Epoch 700/1000, loss=0.0000\n",
    "Epoch 800/1000, loss=0.0000\n",
    "Epoch 900/1000, loss=0.0000\n",
    "Epoch 1000/1000, loss=0.0000\n",
    "Final loss, loss=0.0000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5fcb9-8350-41fa-b38d-8486e236d3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatBot",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
